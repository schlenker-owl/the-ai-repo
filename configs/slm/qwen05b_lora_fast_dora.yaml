# ======================================================================
# Qwen-0.5B  •  DoRA (LoRA with weight decomposition)  •  FAST probe
# ----------------------------------------------------------------------
# WHEN TO USE
# - You want the benefits of LoRA with DoRA’s weight decomposition,
#   which often improves faithfulness/stability with similar parameter cost.
#
# NOTES
# - Keep KL off to isolate DoRA’s effect.
# - Same fast probe levers: steps=60, seq_len=512, ablate=96.
# ======================================================================

base_model: "Qwen/Qwen2.5-0.5B-Instruct"
dataset_path: "data/sft/spirit_chatml.jsonl"
system_prompt: "You are a compassionate, practical spiritual coach. Be concise, kind, and useful."
output_dir: "outputs/qwen05b_fast_dora"

# Fast probe knobs
max_steps: 60
learning_rate: 6.0e-5      # a touch higher than KL lane for crisp loss drop
batch_size: 1
grad_accum: 8
max_seq_len: 512
print_every_steps: 5
log_wandb: false

# LoRA/DoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_mode: "attn_mlp"  # attention + MLP proj → more steering capacity
use_dora: true                # ✅ Enable DoRA

# KL-SFT (off for this lane)
kl_lambda: 0.0
kl_tau: 1.0

# Fast shortcuts
ablate_examples: 96
ablate_seed: 42
max_minutes: 10

# Early stop (train-loss plateau)
early_stop_patience: 15
early_stop_min_delta: 0.003
early_stop_window: 5
early_stop_min_steps: 100

# Eval-loss early stop (off for fast)
eval_early_stop: false
eval_steps: 100
eval_patience: 5

# QoL
max_grad_norm: 1.0

# Post-train quick eval
eval_after_train: false
