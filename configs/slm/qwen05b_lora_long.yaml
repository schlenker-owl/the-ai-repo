base_model: "Qwen/Qwen2.5-0.5B-Instruct"
dataset_path: "data/sft/spirit_alpaca.jsonl"
output_dir: "outputs/qwen05b_lora_long"

# LONG
max_steps: 2000
learning_rate: 3.0e-5   # smaller LR for longer schedule
batch_size: 1
grad_accum: 32
max_seq_len: 1024
print_every_steps: 10
log_wandb: false

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: "auto"

ablate_examples: 0
ablate_seed: 42

eval_after_train: true
dev_path: "data/sft/dev_toy.jsonl"
eval_limit: 40
max_new_tokens: 128
