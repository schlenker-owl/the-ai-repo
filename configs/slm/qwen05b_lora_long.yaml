base_model: "Qwen/Qwen2.5-0.5B-Instruct"
# dataset_path: "data/sft/spirit_alpaca.jsonl"
# dataset_path: "data/sft/spirit_chatml.jsonl"
dataset_path: "data/sft/spirit_chatml_mix.jsonl"
output_dir: "outputs/qwen05b_lora_long"

# LONG
max_steps: 2000
learning_rate: 3.0e-5   # smaller LR for longer schedule
batch_size: 1
grad_accum: 32
max_seq_len: 1024
print_every_steps: 10
log_wandb: false

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: "auto"

ablate_examples: 0
ablate_seed: 42

eval_after_train: true
dev_path: "data/sft/dev_toy.jsonl"
eval_limit: 40
max_new_tokens: 128

# early stop (counts in logging events, not raw steps)
early_stop_patience: 15        # how many consecutive logs w/o improvement
early_stop_min_delta: 0.003    # require at least this loss improvement
early_stop_window: 5           # moving-avg window (logs) to smooth loss
early_stop_min_steps: 100      # donâ€™t early-stop before this many steps

# optional QoL
max_grad_norm: 1.0             # 0 disables; 1.0 is a safe default
